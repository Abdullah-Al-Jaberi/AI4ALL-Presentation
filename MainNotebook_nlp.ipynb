{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Datasets**\n",
        "\n"
      ],
      "metadata": {
        "id": "wIoMgLx0mxJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary packages\n",
        "!pip install gdown\n",
        "!pip install pandas\n",
        "\n",
        "# Download Haiti Dataset\n",
        "!gdown 1lyemu9dz6L1YS-MTNU25KWSN4X0AydXa # Train\n",
        "!gdown 1eZtMBEO3kkpd3WzkPeWR2-6DK2uygLNR # Test\n",
        "\n",
        "# Download Sandy Dataset\n",
        "!gdown 14AG0JK9t6iYR4nvWhxuU6QyBhoOYD-_5 # Train\n",
        "!gdown 1gpKC6Ks0nQDkZ8TOBjKZeblhIJiG520e # Test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5gdx5TSSBmF",
        "outputId": "984550fa-154b-4430-a43a-eac57a3d3611"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lyemu9dz6L1YS-MTNU25KWSN4X0AydXa\n",
            "To: /content/haiti_train.csv\n",
            "100% 172k/172k [00:00<00:00, 64.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eZtMBEO3kkpd3WzkPeWR2-6DK2uygLNR\n",
            "To: /content/haiti_test.csv\n",
            "100% 43.9k/43.9k [00:00<00:00, 51.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14AG0JK9t6iYR4nvWhxuU6QyBhoOYD-_5\n",
            "To: /content/sandy_train.csv\n",
            "100% 155k/155k [00:00<00:00, 61.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gpKC6Ks0nQDkZ8TOBjKZeblhIJiG520e\n",
            "To: /content/sandy_test.csv\n",
            "100% 40.2k/40.2k [00:00<00:00, 53.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load and Combine Datasets**"
      ],
      "metadata": {
        "id": "VDq_LHEsSC9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "haiti_train_df = pd.read_csv('haiti_train.csv')\n",
        "haiti_test_df = pd.read_csv('haiti_test.csv')\n",
        "sandy_train_df = pd.read_csv('sandy_train.csv')\n",
        "sandy_test_df = pd.read_csv('sandy_test.csv')\n",
        "\n",
        "# Combine train and test datasets for Haiti and Sandy\n",
        "haiti_df = pd.concat([haiti_train_df, haiti_test_df], ignore_index=True)\n",
        "sandy_df = pd.concat([sandy_train_df, sandy_test_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "TxNptPyvSHK0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**"
      ],
      "metadata": {
        "id": "fFC_s-p4SZuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values by dropping rows with missing data\n",
        "haiti_df = haiti_df.dropna()\n",
        "sandy_df = sandy_df.dropna()\n",
        "\n",
        "# Remove duplicate entries\n",
        "haiti_df = haiti_df.drop_duplicates()\n",
        "sandy_df = sandy_df.drop_duplicates()"
      ],
      "metadata": {
        "id": "O8FC1FM5SUti"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Labeling and Categorization**"
      ],
      "metadata": {
        "id": "daj5hFt4ShlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to numeric values\n",
        "label_mapping = {'Food': 0, 'Water': 1, 'Energy': 2, 'Medical': 3, 'N/A': 4}\n",
        "\n",
        "haiti_df['Label'] = haiti_df['Label'].map(label_mapping)\n",
        "sandy_df['Label'] = sandy_df['Label'].map(label_mapping)\n"
      ],
      "metadata": {
        "id": "n6WGB-ZkShDY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "W1vTFpUmSmi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Join tokens back to string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "haiti_df['processed_text'] = haiti_df['Text'].apply(preprocess_text)\n",
        "sandy_df['processed_text'] = sandy_df['Text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATDUcn0ZSro7",
        "outputId": "b85c02f8-d8fc-4be3-fca8-1683572695d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature **Engineering**"
      ],
      "metadata": {
        "id": "RZ1R8JAiSufJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "\n",
        "haiti_tfidf = vectorizer.fit_transform(haiti_df['processed_text'])\n",
        "sandy_tfidf = vectorizer.fit_transform(sandy_df['processed_text'])"
      ],
      "metadata": {
        "id": "qMquTFjNSzMS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection (Optional, if needed)"
      ],
      "metadata": {
        "id": "tNtgnXWAS5ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Feature selection using PCA\n",
        "pca = PCA(n_components=100)  # Adjust n_components as needed\n",
        "\n",
        "haiti_tfidf_pca = pca.fit_transform(haiti_tfidf.toarray())\n",
        "sandy_tfidf_pca = pca.fit_transform(sandy_tfidf.toarray())\n"
      ],
      "metadata": {
        "id": "hhPvven9S5Rl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Preprocessed **Datasets**"
      ],
      "metadata": {
        "id": "lkdD5AbkT2eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the TF-IDF vectors to the DataFrame\n",
        "haiti_tfidf_df = pd.DataFrame(haiti_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "sandy_tfidf_df = pd.DataFrame(sandy_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "haiti_preprocessed_df = pd.concat([haiti_df.reset_index(drop=True), haiti_tfidf_df], axis=1)\n",
        "sandy_preprocessed_df = pd.concat([sandy_df.reset_index(drop=True), sandy_tfidf_df], axis=1)\n",
        "\n",
        "# Save the preprocessed Haiti dataset to a CSV file\n",
        "haiti_preprocessed_df.to_csv('haiti_preprocessed.csv', index=False)\n",
        "\n",
        "# Save the preprocessed Sandy dataset to a CSV file\n",
        "sandy_preprocessed_df.to_csv('sandy_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "1IL7RHs6T7RE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Preprocessed Datasets"
      ],
      "metadata": {
        "id": "KCU6y2D4UBYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the preprocessed Haiti dataset\n",
        "files.download('haiti_preprocessed.csv')\n",
        "\n",
        "# Download the preprocessed Sandy dataset\n",
        "files.download('sandy_preprocessed.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "S2MVg-oOUFZi",
        "outputId": "3c4ff710-12be-44a4-afc1-27045bc54c1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e1eebea8-f065-45d0-8987-4c35b0313bed\", \"haiti_preprocessed.csv\", 5584672)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4893f92a-5eed-43de-9e76-3a59f2d89b66\", \"sandy_preprocessed.csv\", 4448487)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}